{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Image Colorization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2UwUgpn5KEb"
      },
      "source": [
        "Just press 'Runtime' -> 'Run all' and the program will start.\n",
        "\n",
        "It's set to use pretrained models, but if training is needed, change the last block of code according to the instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHvZEVOU0emZ",
        "outputId": "c154352c-7de9-42d4-c256-b56630c5a2dc"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
        "from skimage.io import imread, imshow\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageCms\n",
        "from torchsummary import summary\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.vgg import vgg16, vgg19\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "import sys\n",
        "import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "GDRIVE_MOUNT = False\n",
        "models_path = '/content/saved_models/'\n",
        "saved_images_path = '/content/saved_images/'\n",
        "saved_images_test_path = '/content/saved_images_test/'\n",
        "gdrive_saved_models = \"/content/gdrive/MyDrive/Computer_Science/Colab/project/unet_only/\"\n",
        "\n",
        "data_path = 'landscape'\n",
        "\n",
        "# dataset of 328K images\n",
        "if data_path == 'test_256':\n",
        "    if not os.path.isfile('test_256.tar'):\n",
        "        !wget 'http://data.csail.mit.edu/places/places365/test_256.tar'\n",
        "        shutil.unpack_archive(\"test_256.tar\", \"/content/data/\")\n",
        "\n",
        "# dataset of 20K images\n",
        "elif data_path == 'small_test_256':\n",
        "    if not os.path.isfile('small_test_256.zip'):\n",
        "      !gdown --id '1_u1S2C6zuBgeT__QukSUP41sjs8uOg_M'\n",
        "      shutil.unpack_archive(\"small_test_256.zip\", \"/content/data/\")\n",
        "\n",
        "# dataset of landscapes\n",
        "elif data_path == 'landscape':\n",
        "    if not os.path.isfile('landscape_data.zip'):\n",
        "        !gdown --id '1gCMCD1CrTu1QK_KCoSROy6rEFiAfC9QB'\n",
        "        shutil.unpack_archive(\"landscape_data.zip\", \"/content/data/\")\n",
        "    if not os.path.isfile('landscape_test.zip'):\n",
        "        !gdown --id '188faJSg2spTlt25ndD9jddR5DbXTsqQu'\n",
        "        shutil.unpack_archive(\"landscape_test.zip\", \"/content/data/\")\n",
        "\n",
        "if not os.path.exists(saved_images_path):\n",
        "    os.makedirs(saved_images_path)\n",
        "\n",
        "if not os.path.exists(saved_images_test_path):\n",
        "    os.makedirs(saved_images_test_path)\n",
        "\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gCMCD1CrTu1QK_KCoSROy6rEFiAfC9QB\n",
            "To: /content/landscape_data.zip\n",
            "651MB [00:05, 120MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=188faJSg2spTlt25ndD9jddR5DbXTsqQu\n",
            "To: /content/landscape_test.zip\n",
            "7.66MB [00:00, 24.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx2nVVXX0Orl"
      },
      "source": [
        "## Uncomment to also use GDrive, for debugging\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# from google.colab import files\n",
        "\n",
        "# GDRIVE_MOUNT = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLeyVLIV0eh7"
      },
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_WORKERS = 4\n",
        "NUM_EPOCHS = 50\n",
        "ROOT_PATH = \"/content/data/\"\n",
        "IM_SIZE = (512, 512)\n",
        "# IM_SIZE = (256, 256)\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a_cgm_s0ecL"
      },
      "source": [
        "# Utils\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IM_SIZE),\n",
        "    transforms.RandomHorizontalFlip(p=0.5)\n",
        "])\n",
        "\n",
        "default_transform = transforms.Compose([\n",
        "    transforms.Resize(IM_SIZE)\n",
        "])\n",
        "\n",
        "# custom dataset for colorization task\n",
        "class ColorizationDataset(Dataset):\n",
        "    def __init__(self, path, transform=None):\n",
        "        self.transform = transform\n",
        "        self.root_path = path\n",
        "        first = 'Z'\n",
        "        for file in os.listdir(self.root_path):\n",
        "            if file < first:\n",
        "                first = file\n",
        "        self.first_idx = int(first[-12:-4]) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(os.listdir(self.root_path))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index += self.first_idx\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.to_list()\n",
        "        ending = '00000000' + str(index + 1)\n",
        "        path_ending = ending[-8:] + '.jpg'\n",
        "\n",
        "        im_rgb = Image.open(f'{self.root_path}/{path_ending}').convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            im_rgb = self.transform(im_rgb)\n",
        "        # im_rgb = im_rgb.resize(IM_SIZE)\n",
        "        im_rgb = np.array(im_rgb)\n",
        "        im_lab = rgb2lab(im_rgb).astype(\"float32\") # Converting RGB to L*a*b\n",
        "        im_lab = transforms.ToTensor()(im_lab)\n",
        "\n",
        "        # LAB colorspace normalization\n",
        "        l = im_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
        "        ab = im_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
        "        Lab = torch.cat((l, ab), dim=0)\n",
        "\n",
        "        # l = grey, ab = color\n",
        "        return l, ab\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "def load_dataset(path, test = False, shuffle=True):\n",
        "    if test is True:\n",
        "        dataset = ColorizationDataset(path, default_transform)\n",
        "    else:\n",
        "        dataset = ColorizationDataset(path, train_transform)\n",
        "    idxs = list(range(len(dataset)))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idxs)\n",
        "        sampler = torch.utils.data.sampler.SubsetRandomSampler(idxs)\n",
        "    else:\n",
        "        sampler = torch.utils.data.sampler.SequentialSampler(idxs)\n",
        "    loader = torch.utils.data.DataLoader(dataset, sampler=sampler, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "    return loader\n",
        "\n",
        "\n",
        "# Denormalize images\n",
        "def denormalize(images, means, stds):\n",
        "    means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
        "    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
        "    return images * stds + means\n",
        "\n",
        "\n",
        "# Convert lab image to rgb\n",
        "def my_lab2rgb(L, AB, denormalize = True):\n",
        "        L = torch.squeeze(L, dim=0)\n",
        "        AB = torch.squeeze(AB, dim=0)\n",
        "        if denormalize is True:\n",
        "            AB = AB * 110.0\n",
        "            L = (L + 1.0) * 50.0\n",
        "        Lab = torch.cat((L, AB), dim=0)\n",
        "        Lab = Lab.data.cpu().float().numpy()\n",
        "        Lab = np.transpose(Lab.astype(np.float64), (1, 2, 0))\n",
        "        rgb = lab2rgb(Lab)\n",
        "        rgb = np.transpose(rgb.astype(np.float64), (2, 0, 1))\n",
        "        return rgb\n",
        "\n",
        "\n",
        "# Delete contents of provided path\n",
        "def delete_images(path):\n",
        "    files = glob.glob(path + '*')\n",
        "    # print(len(files))\n",
        "    # for f in files:\n",
        "        # os.remove(f)\n",
        "    shutil.rmtree(path, ignore_errors=True)\n",
        "    os.makedirs(path)\n",
        "\n",
        "\n",
        "# Find normalizing parameters\n",
        "def find_mean_std(loader):\n",
        "    mean = 0.\n",
        "    std = 0.\n",
        "    with tqdm.tqdm(total=(len(loader)), file=sys.stdout) as pbar:\n",
        "      for images in loader:\n",
        "        #   l = l.squeeze(dim=0)\n",
        "        #   ab = ab.squeeze(dim=0)\n",
        "        #   Lab = torch.cat((l, ab), dim=0)\n",
        "          batch_samples = images.size(0)\n",
        "          images = images.view(batch_samples, images.size(1), -1)\n",
        "          mean += images.mean(2).sum(0)\n",
        "          std += images.std(2).sum(0)\n",
        "          pbar.update(); \n",
        "    mean /= len(loader.dataset)\n",
        "    std /= len(loader.dataset)\n",
        "    print(mean, std)\n",
        "\n",
        "\n",
        "# Save checkpoint\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "# Load checkpoint\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "# Set whether model requires grad\n",
        "def set_requires_grad(model, requires_grad=True):\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = requires_grad\n",
        "\n",
        "\n",
        "# Class to keep track of loss\n",
        "class LossClassPix2pix:\n",
        "    def __init__(self):\n",
        "        self.loss_D_fake = 0\n",
        "        self.loss_D_real = 0\n",
        "        self.loss_D = 0\n",
        "        self.loss_G_GAN = 0\n",
        "        self.loss_G_L1 = 0\n",
        "        self.loss_G = 0\n",
        "        self.loss_VGG = 0\n",
        "\n",
        "\n",
        "class LossClassCycle:\n",
        "    def __init__(self):\n",
        "        self.loss_D_C_fake = 0\n",
        "        self.loss_D_G_fake = 0\n",
        "        self.loss_D_C_real = 0\n",
        "        self.loss_D_G_real = 0\n",
        "        self.loss_D_C = 0\n",
        "        self.loss_D_G = 0\n",
        "        self.loss_G_C_GAN = 0\n",
        "        self.loss_G_G_GAN = 0\n",
        "        self.loss_G_C_L1 = 0\n",
        "        self.loss_G_G_L1 = 0\n",
        "        self.loss_G_C_Cycle = 0\n",
        "        self.loss_G_G_Cycle = 0\n",
        "        self.loss_G_C = 0\n",
        "        self.loss_G_G = 0\n",
        "        self.loss_VGG = 0\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        self.count, self.avg, self.sum = [0.] * 3\n",
        "        self.avg_list = []\n",
        "        self.clean_list = []\n",
        "    \n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += count * val\n",
        "        self.avg = self.sum / self.count\n",
        "        self.avg_list.append(self.avg)\n",
        "        self.clean_list.append(val)\n",
        "\n",
        "\n",
        "def create_loss_meters_pix2pix():\n",
        "    loss_D_fake = AverageMeter()\n",
        "    loss_D_real = AverageMeter()\n",
        "    loss_D = AverageMeter()\n",
        "    loss_G_GAN = AverageMeter()\n",
        "    loss_G_L1 = AverageMeter()\n",
        "    loss_G = AverageMeter()\n",
        "    loss_VGG = AverageMeter()\n",
        "\n",
        "    return {'loss_D_fake': loss_D_fake,\n",
        "            'loss_D_real': loss_D_real,\n",
        "            'loss_D': loss_D,\n",
        "            'loss_G_GAN': loss_G_GAN,\n",
        "            'loss_G_L1': loss_G_L1,\n",
        "            'loss_G': loss_G,\n",
        "            'loss_VGG': loss_VGG}\n",
        "\n",
        "\n",
        "def create_loss_meters_cycle():\n",
        "    loss_D_C_fake = AverageMeter()\n",
        "    loss_D_G_fake = AverageMeter()\n",
        "    loss_D_C_real = AverageMeter()\n",
        "    loss_D_G_real = AverageMeter()\n",
        "    loss_D_C = AverageMeter()\n",
        "    loss_D_G = AverageMeter()\n",
        "    loss_G_C_GAN = AverageMeter()\n",
        "    loss_G_G_GAN = AverageMeter()\n",
        "    loss_G_C_L1 = AverageMeter()\n",
        "    loss_G_G_L1 = AverageMeter()\n",
        "    loss_G_C_Cycle = AverageMeter()\n",
        "    loss_G_G_Cycle = AverageMeter()\n",
        "    loss_G_C = AverageMeter()\n",
        "    loss_G_G = AverageMeter()\n",
        "    loss_VGG = AverageMeter()\n",
        "    \n",
        "    return {'loss_D_C_fake': loss_D_C_fake,\n",
        "            'loss_D_G_fake': loss_D_G_fake,\n",
        "            'loss_D_C_real': loss_D_C_real,\n",
        "            'loss_D_G_real': loss_D_G_real,\n",
        "            'loss_D_C': loss_D_C,\n",
        "            'loss_D_G': loss_D_G,\n",
        "            'loss_G_C_GAN': loss_G_C_GAN,\n",
        "            'loss_G_G_GAN': loss_G_G_GAN,\n",
        "            'loss_G_C_L1': loss_G_C_L1,\n",
        "            'loss_G_G_L1': loss_G_G_L1,\n",
        "            'loss_G_C_Cycle': loss_G_C_Cycle,\n",
        "            'loss_G_G_Cycle': loss_G_G_Cycle,\n",
        "            'loss_G_C': loss_G_C,\n",
        "            'loss_G_G': loss_G_G,\n",
        "            'loss_VGG': loss_VGG}\n",
        "\n",
        "\n",
        "def update_losses(losses, losses_dict, count):\n",
        "    for loss_name, loss_meter in losses_dict.items():\n",
        "        loss = getattr(losses, loss_name)\n",
        "        if hasattr(loss, 'item'):\n",
        "            loss_meter.update(loss.item(), count=count)\n",
        "        else:\n",
        "            loss_meter.update(loss, count=count)\n",
        "\n",
        "\n",
        "def log_results(loss_meter_dict):\n",
        "    print('\\n')\n",
        "    for loss_name, loss_meter in loss_meter_dict.items():\n",
        "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijpVni5HX5Ft"
      },
      "source": [
        "# Generator model - old\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
        "            if down\n",
        "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
        "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, num_features = 64, num_residuals = 1):\n",
        "        super().__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n",
        "            nn.InstanceNorm2d(num_features),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.down_blocks = nn.ModuleList(\n",
        "            [\n",
        "                ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n",
        "                ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n",
        "                ConvBlock(num_features*4, num_features*8, kernel_size=3, stride=2, padding=1),\n",
        "                ConvBlock(num_features*8, num_features*16, kernel_size=3, stride=2, padding=1),\n",
        "                ConvBlock(num_features*16, num_features*32, kernel_size=3, stride=2, padding=1),\n",
        "            ]\n",
        "        )\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(num_features*32) for _ in range(num_residuals)]\n",
        "        )\n",
        "        self.up_blocks = nn.ModuleList(\n",
        "            [\n",
        "                ConvBlock(num_features*32, num_features*16, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                ConvBlock(num_features*16, num_features*8, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                ConvBlock(num_features*8, num_features*4, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                ConvBlock(num_features*2, num_features*1, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            ]\n",
        "        )\n",
        "        self.last = nn.Conv2d(num_features*1, output_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n",
        "\n",
        "    def forward(self, x, L = None):\n",
        "        if L is not None:\n",
        "            return L\n",
        "        x = self.initial(x)\n",
        "        for layer in self.down_blocks:\n",
        "            x = layer(x)\n",
        "        x = self.res_blocks(x)\n",
        "        for layer in self.up_blocks:\n",
        "            x = layer(x)\n",
        "        return torch.tanh(self.last(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXOG7GsmPxj9"
      },
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "            if down\n",
        "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.down = down\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return self.dropout(x) if self.use_dropout else x\n",
        "\n",
        "\n",
        "class GeneratorPix2pix(nn.Module):\n",
        "    def __init__(self, input_channels=3, output_channels=3, features=64):\n",
        "        super().__init__()\n",
        "        self.initial_down = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
        "        self.down2 = Block(\n",
        "            features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False\n",
        "        )\n",
        "        self.down3 = Block(\n",
        "            features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
        "        )\n",
        "        self.down4 = Block(\n",
        "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
        "        )\n",
        "        self.down5 = Block(\n",
        "            features * 8, features * 16, down=True, act=\"leaky\", use_dropout=False\n",
        "        )\n",
        "        self.down6 = Block(\n",
        "            features * 16, features * 16, down=True, act=\"leaky\", use_dropout=False\n",
        "        )\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features * 16, features * 16, 4, 2, 1), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.up1 = Block(features * 16, features * 16, down=False, act=\"relu\", use_dropout=True)\n",
        "        self.up2 = Block(\n",
        "            features * 16 * 2, features * 16, down=False, act=\"relu\", use_dropout=True\n",
        "        )\n",
        "        self.up3 = Block(\n",
        "            features * 16 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
        "        )\n",
        "        self.up4 = Block(\n",
        "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
        "        )\n",
        "        self.up5 = Block(\n",
        "            features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False\n",
        "        )\n",
        "        self.up6 = Block(\n",
        "            features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False\n",
        "        )\n",
        "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
        "        self.final_up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features * 2, output_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.initial_down(x)\n",
        "        d2 = self.down1(d1)\n",
        "        d3 = self.down2(d2)\n",
        "        d4 = self.down3(d3)\n",
        "        d5 = self.down4(d4)\n",
        "        d6 = self.down5(d5)\n",
        "        d7 = self.down6(d6)\n",
        "        bottleneck = self.bottleneck(d7)\n",
        "        up1 = self.up1(bottleneck)\n",
        "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
        "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
        "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
        "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
        "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
        "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
        "        return self.final_up(torch.cat([up7, d1], 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kImYJnn25onb"
      },
      "source": [
        "# Discriminator model\n",
        "\n",
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=True, padding_mode=\"reflect\"),\n",
        "            nn.InstanceNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                features[0],\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                padding_mode=\"reflect\",\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        layers = []\n",
        "        in_channels = features[0]\n",
        "        for feature in features[1:]:\n",
        "            layers.append(DisBlock(in_channels, feature, stride=1 if feature==features[-1] else 2))\n",
        "            in_channels = feature\n",
        "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial(x)\n",
        "        return torch.sigmoid(self.model(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfW5cYaU6YpL"
      },
      "source": [
        "# VGG perceptual loss\n",
        "\n",
        "class VGGPerceptualLoss(torch.nn.Module):\n",
        "    def __init__(self, resize=True):\n",
        "        super(VGGPerceptualLoss, self).__init__()\n",
        "        blocks = []\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n",
        "        for bl in blocks:\n",
        "            for p in bl:\n",
        "                p.requires_grad = False\n",
        "        self.blocks = torch.nn.ModuleList(blocks)\n",
        "        self.transform = torch.nn.functional.interpolate\n",
        "        self.resize = resize\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, input, target, feature_layers=[0, 1, 2, 3], style_layers=[]):\n",
        "        if input.shape[1] != 3:\n",
        "            input = input.repeat(1, 3, 1, 1)\n",
        "            target = target.repeat(1, 3, 1, 1)\n",
        "        input = (input-self.mean) / self.std\n",
        "        target = (target-self.mean) / self.std\n",
        "        if self.resize:\n",
        "            input = self.transform(input, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "            target = self.transform(target, mode='bilinear', size=(224, 224), align_corners=False)\n",
        "        loss = 0.0\n",
        "        x = input\n",
        "        y = target\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            y = block(y)\n",
        "            if i in feature_layers:\n",
        "                loss += torch.nn.functional.l1_loss(x, y)\n",
        "            if i in style_layers:\n",
        "                act_x = x.reshape(x.shape[0], x.shape[1], -1)\n",
        "                act_y = y.reshape(y.shape[0], y.shape[1], -1)\n",
        "                gram_x = act_x @ act_x.permute(0, 2, 1)\n",
        "                gram_y = act_y @ act_y.permute(0, 2, 1)\n",
        "                loss += torch.nn.functional.l1_loss(gram_x, gram_y)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y9uaU9OtUs1"
      },
      "source": [
        "# Train function for U-Net only\n",
        "\n",
        "def train_fn_unet(gen_C, opt_G, loader, l1_crit, gan_crit, epoch):\n",
        "    \n",
        "    losses_dict = create_loss_meters_pix2pix()\n",
        "    vgg_crit = VGGPerceptualLoss().to(DEVICE)\n",
        "\n",
        "    with tqdm.tqdm(total=(len(loader)), file=sys.stdout, leave=True, position=0) as pbar:\n",
        "        for idx, (l, ab) in enumerate(loader, start=1):\n",
        "            losses = LossClassPix2pix()\n",
        "            l = l.to(DEVICE)\n",
        "            ab = ab.to(DEVICE)\n",
        "            \n",
        "            fake_color = gen_C(l)\n",
        "            fake_color_im = torch.cat((l, fake_color), 1)\n",
        "            real_color_im = torch.cat((l, ab), 1)\n",
        "\n",
        "            \n",
        "            # real_im = np.transpose(my_lab2rgb(l, ab), (1,2,0))\n",
        "            # fake_im = np.transpose(my_lab2rgb(l, fake_color), (1,2,0))\n",
        "            # fake = torch.from_numpy(np.transpose(fake_im, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            # real = torch.from_numpy(np.transpose(real_im, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            # losses.loss_VGG = vgg_crit(fake.float(), real.float())\n",
        "\n",
        "            fake = fake_color_im.to(DEVICE)\n",
        "            real = real_color_im.to(DEVICE)\n",
        "            losses.loss_VGG = vgg_crit(fake.float(), real.float())\n",
        "\n",
        "            # Train Generator\n",
        "            gen_C.train()\n",
        "            opt_G.zero_grad()\n",
        "            losses.loss_G_L1 = l1_crit(fake_color, ab)\n",
        "            losses.loss_G = losses.loss_G_L1\n",
        "            losses.loss_G.backward(retain_graph=False)\n",
        "            opt_G.step()\n",
        "            gen_C.eval()\n",
        "\n",
        "            # Update losses\n",
        "            update_losses(losses, losses_dict, count=l.size(0))\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                log_results(losses_dict)\n",
        "                im_rgb_np = my_lab2rgb(l, fake_color)\n",
        "                im_rgb = torch.from_numpy(im_rgb_np)\n",
        "                if not os.path.exists(saved_images_path + str(epoch)):\n",
        "                    os.makedirs(saved_images_path + str(epoch))\n",
        "                save_image(im_rgb, saved_images_path + str(epoch) + f\"/{idx}_rgb_color.png\")\n",
        "                save_image(torch.from_numpy(my_lab2rgb(l, ab)), saved_images_path + str(epoch) + f\"/{idx}_rgb_real.png\")\n",
        "                \n",
        "            pbar.update();\n",
        "    return losses_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHxhC8Rl28iK"
      },
      "source": [
        "# Train function for Pix2Pix\n",
        "\n",
        "def train_fn_pix2pix(disc_C, gen_C, loader, opt_D, opt_G, l1_crit, gan_crit, epoch):\n",
        "    C_reals = 0\n",
        "    C_fakes = 0\n",
        "    lambda_l1 = 5\n",
        "    \n",
        "    losses_dict = create_loss_meters_pix2pix()\n",
        "    vgg_crit = VGGPerceptualLoss().to(DEVICE)\n",
        "\n",
        "    with tqdm.tqdm(total=(len(loader)), file=sys.stdout, leave=True, position=0) as pbar:\n",
        "        for idx, (l, ab) in enumerate(loader, start=1):\n",
        "            losses = LossClassPix2pix()\n",
        "            l = l.to(DEVICE)\n",
        "            ab = ab.to(DEVICE)\n",
        "            \n",
        "            fake_color = gen_C(l)\n",
        "            fake_color_im = torch.cat((l, fake_color), 1)\n",
        "            real_color_im = torch.cat((l, ab), 1)\n",
        "\n",
        "            \n",
        "            # real_im = np.transpose(my_lab2rgb(l, ab), (1,2,0))\n",
        "            # fake_im = np.transpose(my_lab2rgb(l, fake_color), (1,2,0))\n",
        "            # fake = torch.from_numpy(np.transpose(fake_im, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            # real = torch.from_numpy(np.transpose(real_im, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            # losses.loss_VGG = vgg_crit(fake.float(), real.float())\n",
        "\n",
        "            fake = fake_color_im.to(DEVICE)\n",
        "            real = real_color_im.to(DEVICE)\n",
        "            losses.loss_VGG = vgg_crit(fake.float(), real.float())\n",
        "\n",
        "            # Train Discriminator\n",
        "            disc_C.train()\n",
        "            set_requires_grad(disc_C, True)\n",
        "            opt_D.zero_grad()\n",
        "            real_pred = disc_C(real_color_im)\n",
        "            fake_pred = disc_C(fake_color_im.detach())\n",
        "            losses.loss_D_real = gan_crit(real_pred, torch.ones_like(real_pred))\n",
        "            losses.loss_D_fake = gan_crit(fake_pred, torch.zeros_like(fake_pred))\n",
        "            losses.loss_D = (losses.loss_D_fake + losses.loss_D_real) / 2\n",
        "            losses.loss_D.backward(retain_graph=False)\n",
        "            opt_D.step()\n",
        "            disc_C.eval()\n",
        "\n",
        "            # Train Generator\n",
        "            gen_C.train()\n",
        "            set_requires_grad(disc_C, False)\n",
        "            opt_G.zero_grad()\n",
        "            fake_pred = disc_C(fake_color_im.detach())\n",
        "            losses.loss_G_GAN = gan_crit(fake_pred, torch.ones_like(fake_pred))\n",
        "            losses.loss_G_L1 = l1_crit(fake_color, ab) * lambda_l1\n",
        "            losses.loss_G = losses.loss_G_GAN + losses.loss_G_L1\n",
        "            losses.loss_G.backward(retain_graph=False)\n",
        "            opt_G.step()\n",
        "            gen_C.eval()\n",
        "\n",
        "            # Update losses\n",
        "            C_reals += real_pred.mean().item()\n",
        "            C_fakes += fake_pred.mean().item()\n",
        "            update_losses(losses, losses_dict, count=l.size(0))\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                log_results(losses_dict)\n",
        "                im_rgb_np = my_lab2rgb(l, fake_color)\n",
        "                im_rgb = torch.from_numpy(im_rgb_np)\n",
        "                if not os.path.exists(saved_images_path + str(epoch)):\n",
        "                    os.makedirs(saved_images_path + str(epoch))\n",
        "                save_image(im_rgb, saved_images_path + str(epoch) + f\"/{idx}_rgb_color.png\")\n",
        "                save_image(torch.from_numpy(my_lab2rgb(l, ab)), saved_images_path + str(epoch) + f\"/{idx}_rgb_real.png\")\n",
        "                \n",
        "            pbar.update(); pbar.set_postfix(C_real=C_reals/(idx+1), C_fake=C_fakes/(idx+1))\n",
        "    return losses_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ignZVjNDLY79"
      },
      "source": [
        "# Train function for CycleGAN\n",
        "\n",
        "def train_fn_cycle(disc_C, disc_G, gen_C, gen_G, loader, opt_D, opt_G, l1_crit, gan_crit, d_scaler, g_scaler, epoch):\n",
        "    C_reals = 0\n",
        "    C_fakes = 0\n",
        "    lambda_gan = 5\n",
        "    lambda_l1 = 1\n",
        "    lambda_cycle = 10\n",
        "    \n",
        "    losses_dict = create_loss_meters_cycle()\n",
        "    vgg_crit = VGGPerceptualLoss().to(DEVICE)\n",
        "\n",
        "    with tqdm.tqdm(total=(len(loader)), file=sys.stdout, leave=True, position=0) as pbar:\n",
        "        for idx, (l, ab) in enumerate(loader, start=1):\n",
        "            losses = LossClassCycle()\n",
        "            l = l.to(DEVICE)\n",
        "            ab = ab.to(DEVICE)\n",
        "            \n",
        "            fake_color = gen_C(l)\n",
        "            fake_color_im = torch.cat((l, fake_color), 1)\n",
        "            real_color_im = torch.cat((l, ab), 1)\n",
        "\n",
        "            fake = fake_color_im.to(DEVICE)\n",
        "            real = real_color_im.to(DEVICE)\n",
        "            losses.loss_VGG = vgg_crit(fake.float(), real.float())\n",
        "        \n",
        "            fake_grey = gen_G(ab)\n",
        "            fake_grey_im = torch.cat((fake_grey.detach(), ab), 1)\n",
        "\n",
        "            # Train Discriminators\n",
        "            opt_D.zero_grad()\n",
        "            # Color\n",
        "            disc_C.train()\n",
        "            set_requires_grad(disc_C, True)\n",
        "            real_pred = disc_C(real_color_im)\n",
        "            fake_pred = disc_C(fake_color_im.detach())\n",
        "            losses.loss_D_C_real = gan_crit(real_pred, torch.ones_like(real_pred))\n",
        "            losses.loss_D_C_fake = gan_crit(fake_pred, torch.zeros_like(fake_pred))\n",
        "            losses.loss_D_C = (losses.loss_D_C_fake + losses.loss_D_C_real) / 2\n",
        "            losses.loss_D_C.backward(retain_graph=True)\n",
        "\n",
        "            # Grey\n",
        "            disc_G.train()\n",
        "            set_requires_grad(disc_G, True)\n",
        "            real_pred = disc_G(real_color_im)\n",
        "            fake_pred = disc_G(fake_grey_im.detach())\n",
        "            losses.loss_D_G_real = gan_crit(real_pred, torch.ones_like(real_pred))\n",
        "            losses.loss_D_G_fake = gan_crit(fake_pred, torch.zeros_like(fake_pred))\n",
        "            losses.loss_D_G = (losses.loss_D_G_fake + losses.loss_D_G_real) / 2\n",
        "            losses.loss_D_G.backward(retain_graph=False)\n",
        "            opt_D.step()\n",
        "            disc_C.eval()\n",
        "            disc_G.eval()\n",
        "\n",
        "            # Train Generators\n",
        "            opt_G.zero_grad()\n",
        "            # Color\n",
        "            gen_C.train()\n",
        "            set_requires_grad(disc_C, False)\n",
        "            fake_pred = disc_C(fake_color_im.detach())\n",
        "            losses.loss_G_C_GAN = gan_crit(fake_pred, torch.ones_like(fake_pred))\n",
        "            losses.loss_G_C_L1 = l1_crit(fake_color, ab)\n",
        "            cycle_color = gen_C(fake_grey)\n",
        "            losses.loss_G_C_Cycle = l1_crit(cycle_color, ab)\n",
        "            losses.loss_G_C = losses.loss_G_C_GAN * lambda_gan + losses.loss_G_C_L1 * lambda_l1 + losses.loss_G_C_Cycle * lambda_cycle\n",
        "            losses.loss_G_C.backward(retain_graph=True)\n",
        "\n",
        "            # Grey\n",
        "            gen_G.train()\n",
        "            set_requires_grad(disc_G, False)\n",
        "            fake_pred = disc_G(fake_grey_im.detach())\n",
        "            losses.loss_G_G_GAN = gan_crit(fake_pred, torch.ones_like(fake_pred))\n",
        "            losses.loss_G_G_L1 = l1_crit(fake_grey, l)\n",
        "            cycle_grey = gen_G(fake_color)\n",
        "            losses.loss_G_G_Cycle = l1_crit(cycle_grey, l)\n",
        "            losses.loss_G_G = losses.loss_G_G_GAN * lambda_gan + losses.loss_G_G_L1 * lambda_l1 + losses.loss_G_G_Cycle * lambda_cycle\n",
        "            losses.loss_G_G.backward(retain_graph=False)\n",
        "            opt_G.step()\n",
        "            gen_C.eval()\n",
        "            gen_C.eval()\n",
        "\n",
        "            # Update losses\n",
        "            C_reals += real_pred.mean().item()\n",
        "            C_fakes += fake_pred.mean().item()\n",
        "            update_losses(losses, losses_dict, count=l.size(0))\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                log_results(losses_dict)\n",
        "                im_rgb_np = my_lab2rgb(l, fake_color)\n",
        "                im_rgb = torch.from_numpy(im_rgb_np)\n",
        "                if not os.path.exists(saved_images_path + str(epoch)):\n",
        "                    os.makedirs(saved_images_path + str(epoch))\n",
        "                save_image(im_rgb, saved_images_path + str(epoch) + f\"/{idx}_rgb_color.png\")\n",
        "                save_image(torch.from_numpy(my_lab2rgb(l, ab)), saved_images_path + str(epoch) + f\"/{idx}_rgb_real.png\")\n",
        "                \n",
        "            pbar.update(); pbar.set_postfix(C_real=C_reals/(idx+1), C_fake=C_fakes/(idx+1))\n",
        "    return losses_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZBbsZFHsrDB"
      },
      "source": [
        "# Init training for U-Net only\n",
        "\n",
        "def train_unet(dataset_name, reload = False):\n",
        "    delete_images(saved_images_path) \n",
        "\n",
        "    total_losses = []\n",
        "\n",
        "    gen_C = GeneratorPix2pix(input_channels=1, output_channels=2).to(DEVICE)\n",
        "\n",
        "    summary(gen_C, (1, 256, 256))\n",
        "\n",
        "    opt_G = optim.Adam(\n",
        "        gen_C.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    if reload:\n",
        "        load_checkpoint(gdrive_saved_models + 'CHECKPOINT_GEN_C', gen_C, opt_G, LEARNING_RATE)\n",
        "\n",
        "    l1_crit = nn.L1Loss()\n",
        "    gan_crit = nn.MSELoss()\n",
        "\n",
        "    loader = load_dataset(ROOT_PATH + dataset_name)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        losses = train_fn_unet(gen_C, opt_G, loader, l1_crit, gan_crit, epoch)\n",
        "        total_losses.append(losses)\n",
        "        folder_origin = saved_images_path + str(epoch)\n",
        "        folder_destination = gdrive_saved_models + 'images/'\n",
        "\n",
        "        if GDRIVE_MOUNT:\n",
        "          !cp -av $folder_origin $folder_destination\n",
        "\n",
        "          pickle.dump(total_losses, open(gdrive_saved_models + \"losses.p\", \"wb\" ))\n",
        "\n",
        "          torch.save(gen_C, gdrive_saved_models + 'CHECKPOINT_GEN_C_model')\n",
        "\n",
        "        pickle.dump(total_losses, open(\"losses.p\", \"wb\" ))\n",
        "        torch.save(gen_C, 'CHECKPOINT_GEN_C_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bti87LfV28TJ"
      },
      "source": [
        "# Init training for Pix2Pix\n",
        "\n",
        "def train_pix2pix(dataset_name, reload = False):\n",
        "    delete_images(saved_images_path) \n",
        "\n",
        "    total_losses = []\n",
        "\n",
        "    gen_C = GeneratorPix2pix(input_channels=1, output_channels=2).to(DEVICE)\n",
        "    disc_C = PatchDiscriminator(in_channels=3).to(DEVICE)\n",
        "\n",
        "    summary(gen_C, (1, 256, 256))\n",
        "\n",
        "    opt_D = optim.Adam(\n",
        "        disc_C.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    opt_G = optim.Adam(\n",
        "        gen_C.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    if reload:\n",
        "        load_checkpoint(gdrive_saved_models + 'CHECKPOINT_GEN_C', gen_C, opt_G, LEARNING_RATE)\n",
        "        load_checkpoint(gdrive_saved_models + 'CHECKPOINT_DISC_C', disc_C, opt_D, LEARNING_RATE)\n",
        "\n",
        "    l1_crit = nn.L1Loss()\n",
        "    gan_crit = nn.MSELoss()\n",
        "\n",
        "    loader = load_dataset(ROOT_PATH + dataset_name)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        losses = train_fn_pix2pix(disc_C, gen_C, loader, opt_D, opt_G, l1_crit, gan_crit, epoch)\n",
        "        total_losses.append(losses)\n",
        "        folder_origin = saved_images_path + str(epoch)\n",
        "        folder_destination = gdrive_saved_models + 'images/'\n",
        "\n",
        "        if GDRIVE_MOUNT:\n",
        "          !cp -av $folder_origin $folder_destination\n",
        "\n",
        "          pickle.dump(total_losses, open(gdrive_saved_models + \"losses.p\", \"wb\" ))\n",
        "\n",
        "          torch.save(gen_C, gdrive_saved_models + 'CHECKPOINT_GEN_C_model')\n",
        "          # save_checkpoint(gen_C, opt_G, filename=gdrive_saved_models + 'CHECKPOINT_GEN_C')\n",
        "          # save_checkpoint(disc_C, opt_D, filename=gdrive_saved_models + 'CHECKPOINT_DISC_C')\n",
        "        \n",
        "        pickle.dump(total_losses, open(\"losses.p\", \"wb\" ))\n",
        "\n",
        "        torch.save(gen_C, 'CHECKPOINT_GEN_C_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF4x-QY6K-6h"
      },
      "source": [
        "# Init training for CycleGAN\n",
        "\n",
        "def train_cycle(dataset_name, reload = False):\n",
        "    delete_images(saved_images_path) \n",
        "\n",
        "    total_losses = []\n",
        "\n",
        "    gen_C = GeneratorPix2pix(input_channels=1, output_channels=2).to(DEVICE)\n",
        "    gen_G = GeneratorPix2pix(input_channels=2, output_channels=1).to(DEVICE)\n",
        "\n",
        "    disc_C = PatchDiscriminator(in_channels=3).to(DEVICE)\n",
        "    disc_G = PatchDiscriminator(in_channels=3).to(DEVICE)\n",
        "\n",
        "    opt_D = optim.Adam(\n",
        "        list(disc_C.parameters()) + list(disc_G.parameters()),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    opt_G = optim.Adam(\n",
        "        list(gen_C.parameters()) + list(gen_G.parameters()),\n",
        "        lr=LEARNING_RATE,\n",
        "        betas=(0.5, 0.999),\n",
        "    )\n",
        "\n",
        "    if reload:\n",
        "        load_checkpoint(gdrive_saved_models + 'CHECKPOINT_GEN_C', gen_C, opt_G, LEARNING_RATE)\n",
        "        load_checkpoint(gdrive_saved_models + 'CHECKPOINT_DISC_C', disc_C, opt_D, LEARNING_RATE)\n",
        "\n",
        "    l1_crit = nn.L1Loss()\n",
        "    gan_crit = nn.MSELoss()\n",
        "    # gan_crit = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    loader = load_dataset(ROOT_PATH + dataset_name)\n",
        "\n",
        "    g_scaler = torch.cuda.amp.GradScaler()\n",
        "    d_scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        losses = train_fn_cycle(disc_C, disc_G, gen_C, gen_G, loader, opt_D, opt_G, l1_crit, gan_crit, d_scaler, g_scaler, epoch)\n",
        "        total_losses.append(losses)\n",
        "        folder_origin = saved_images_path + str(epoch)\n",
        "        folder_destination = gdrive_saved_models + 'images/'\n",
        "        !cp -av $folder_origin $folder_destination\n",
        "\n",
        "        pickle.dump(total_losses, open(gdrive_saved_models + \"losses.p\", \"wb\" ))\n",
        "\n",
        "        torch.save(gen_C, gdrive_saved_models + 'CHECKPOINT_GEN_C_model')\n",
        "        # save_checkpoint(gen_C, opt_G, filename=gdrive_saved_models + 'CHECKPOINT_GEN_C')\n",
        "        # save_checkpoint(disc_C, opt_D, filename=gdrive_saved_models + 'CHECKPOINT_DISC_C')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFP0LEtZ28Y4"
      },
      "source": [
        "# Test function\n",
        "\n",
        "def test_fn(model_path, test_path, gdrive_saved_images_test = None):\n",
        "    delete_images(saved_images_test_path)\n",
        "\n",
        "    # load model checkpoint\n",
        "    else:\n",
        "        if TRAINED_MODEL == 'Pix2Pix':\n",
        "            if not os.path.isfile(\"/content/trained_gen_c_\" + TRAINED_MODEL):\n",
        "                !gdown --id '1d1PV3fg3bCoVwRhU3iyBMXYMwW7LuuiO'\n",
        "        if TRAINED_MODEL == 'CycleGAN':\n",
        "            if not os.path.isfile(\"/content/trained_gen_c_\" + TRAINED_MODEL):\n",
        "                !gdown --id '1R3nCUpRl2CdW0-4_BVd7CCirQ1ktuoFv'\n",
        "        if TRAINED_MODEL == 'U-Net':\n",
        "            if not os.path.isfile(\"/content/trained_gen_c_\" + TRAINED_MODEL):\n",
        "                !gdown --id '1imp1BKKyPu4rIPch0CHkBUh-1Fp8SPcG'\n",
        "        gen_C = torch.load(\"/content/trained_gen_c\" + TRAINED_MODEL, map_location=torch.device(DEVICE))\n",
        "    gen_C.eval()\n",
        "\n",
        "    # load test dataset\n",
        "    loader = load_dataset(test_path, test = True, shuffle=False)\n",
        "\n",
        "    # VGG critic\n",
        "    vgg_crit = VGGPerceptualLoss().to(DEVICE)\n",
        "\n",
        "    with tqdm.tqdm(total=(len(loader)), file=sys.stdout, leave=True, position=0) as pbar:\n",
        "        for idx, (l, ab) in enumerate(loader, start=1):\n",
        "            grey = torch.squeeze(l)\n",
        "            l = l.to(DEVICE)\n",
        "            ab = ab.to(DEVICE)\n",
        "\n",
        "            fake_color = gen_C(l)\n",
        "\n",
        "            real_im = np.transpose(my_lab2rgb(l, ab), (1,2,0))\n",
        "            fake_im = np.transpose(my_lab2rgb(l, fake_color), (1,2,0))\n",
        "            grey_im = grey\n",
        "\n",
        "            fake = torch.from_numpy(np.transpose(fake_im, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            real = torch.from_numpy(np.transpose(real_im, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "\n",
        "            vgg_loss = vgg_crit(fake.float(), real.float())\n",
        "\n",
        "            f, axarr = plt.subplots(figsize=(40,20),nrows=1,ncols=3)\n",
        "            plt.sca(axarr[0]); \n",
        "            plt.imshow(grey_im, cmap='gray'); plt.title('Grey Image', fontsize=20)\n",
        "            plt.sca(axarr[1]); \n",
        "            plt.imshow(fake_im); plt.title('Generated Color', fontsize=20); plt.xlabel('VGG loss: ' + str(vgg_loss.item()), fontsize=20)\n",
        "            plt.sca(axarr[2]); \n",
        "            plt.imshow(real_im); plt.title('Original Image', fontsize=20)\n",
        "            plt.savefig(saved_images_test_path + 'im_' + str(idx))\n",
        "\n",
        "            if GDRIVE_MOUNT:\n",
        "                file_name = saved_images_test_path + 'im_' + str(idx) + \".png\"\n",
        "                !cp -av $file_name $gdrive_saved_images_test\n",
        "            pbar.update();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtcK_rePquhj"
      },
      "source": [
        "# test for 2 models\n",
        "\n",
        "def test_fn_2(test_path, gdrive_saved_images_test = None, first = 'CycleGAN', second = 'Pix2Pix'):\n",
        "    delete_images(saved_images_test_path)\n",
        "    model_addresses = {'CycleGAN':'17B5wxLCGNGfXoBtp05-Q3214Ebkwgmsm',\n",
        "                       'Pix2Pix':'1d1PV3fg3bCoVwRhU3iyBMXYMwW7LuuiO',\n",
        "                       'U-Net':'1h3KfiILcgQ6_H5-3wvAVnO-Y0oE46kft',\n",
        "                       'U-Net_2':'1IrS2tZlrL8axqR4qMtZJPTs_jM4LCcsE',\n",
        "                       'Pix2Pix_2':'1Gt4PEMW8fjzH2p3HRZOZiN4FdqB68ykE'}\n",
        "\n",
        "    if not os.path.isfile(\"/content/trained_gen_c_\" + first):\n",
        "        !gdown --id {model_addresses[first]}\n",
        "    if not os.path.isfile(\"/content/trained_gen_c_\" + second):\n",
        "        !gdown --id {model_addresses[second]}\n",
        "    gen_C1 = torch.load(\"/content/trained_gen_c_\" + first, map_location=torch.device(DEVICE))\n",
        "    gen_C2 = torch.load(\"/content/trained_gen_c_\" + second, map_location=torch.device(DEVICE))\n",
        "    gen_C1.eval()\n",
        "    gen_C2.eval()\n",
        "    # load test dataset\n",
        "    loader = load_dataset(test_path, test = True, shuffle=False)\n",
        "\n",
        "    # VGG critic\n",
        "    vgg_crit = VGGPerceptualLoss().to(DEVICE)\n",
        "\n",
        "    with tqdm.tqdm(total=(len(loader)), file=sys.stdout, leave=True, position=0) as pbar:\n",
        "        for idx, (l, ab) in enumerate(loader, start=1):\n",
        "            grey = torch.squeeze(l)\n",
        "            l = l.to(DEVICE)\n",
        "            ab = ab.to(DEVICE)\n",
        "\n",
        "            fake_color_1 = gen_C1(l)\n",
        "            fake_color_2 = gen_C2(l)\n",
        "\n",
        "            real_im = np.transpose(my_lab2rgb(l, ab), (1,2,0))\n",
        "            fake_im_1 = np.transpose(my_lab2rgb(l, fake_color_1), (1,2,0))\n",
        "            fake_im_2 = np.transpose(my_lab2rgb(l, fake_color_2), (1,2,0))\n",
        "            grey_im = grey\n",
        "\n",
        "            real = torch.from_numpy(np.transpose(real_im, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            fake = torch.from_numpy(np.transpose(fake_im_1, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            vgg_loss_1 = vgg_crit(fake.float(), real.float())\n",
        "            fake = torch.from_numpy(np.transpose(fake_im_2, (2,0,1))).unsqueeze(dim=0).to(DEVICE)\n",
        "            vgg_loss_2 = vgg_crit(fake.float(), real.float())\n",
        "\n",
        "            f, axarr = plt.subplots(figsize=(40,20),nrows=1,ncols=4)\n",
        "            plt.sca(axarr[0]); \n",
        "            plt.imshow(grey_im, cmap='gray'); plt.title('Grey Image', fontsize=20)\n",
        "            plt.sca(axarr[1]); \n",
        "            plt.imshow(fake_im_1); plt.title(first, fontsize=20); plt.xlabel('VGG loss: ' + str(vgg_loss_1.item()), fontsize=20)\n",
        "            plt.sca(axarr[2]); \n",
        "            plt.imshow(fake_im_2); plt.title(second, fontsize=20); plt.xlabel('VGG loss: ' + str(vgg_loss_2.item()), fontsize=20)\n",
        "            plt.sca(axarr[3]); \n",
        "            plt.imshow(real_im); plt.title('Original Image', fontsize=20)\n",
        "            plt.savefig(saved_images_test_path + 'im_' + str(idx), fontsize=20)\n",
        "\n",
        "            if GDRIVE_MOUNT:\n",
        "                file_name = saved_images_test_path + 'im_' + str(idx) + \".png\"\n",
        "                !cp -av $file_name $gdrive_saved_images_test\n",
        "            pbar.update();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BYzgzq_S28OY"
      },
      "source": [
        "# Main\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_name = 'landscape'\n",
        "    test_dataset = 'landscape_test'\n",
        "    model_path = gdrive_saved_models + \"CHECKPOINT_GEN_C_model\"\n",
        "    gdrive_saved_images_test = gdrive_saved_models + 'images/test/'\n",
        "    \n",
        "    ## To train a new model, uncomment these lines - choose train_cycle for CycleGAN or train_pix2pix for Pix2Pix\n",
        "    # IM_SIZE = (256, 256)\n",
        "    # train_pix2pix(dataset_name, False)\n",
        "    # train_cycle(dataset_name, False)\n",
        "    # train_unet(dataset_name, False)\n",
        "\n",
        "    ## To test only one model, uncomment these lines.\n",
        "    ## For Pix2Pix model, set TRAINED_MODEL = 'Pix2Pix'\n",
        "    ## For CycleGAN model, set TRAINED_MODEL = 'CycleGAN'\n",
        "    ## For U-Net model, set TRAINED_MODEL = 'U-Net'\n",
        "    # TRAINED_MODEL = 'Pix2Pix'\n",
        "    # IM_SIZE = (512, 512)\n",
        "    # test_fn(model_path = model_path, test_path = ROOT_PATH + test_dataset, gdrive_saved_images_test = gdrive_saved_images_test)\n",
        "\n",
        "    ## To test CycleGAN vs Pix2Pix models, uncomment these lines\n",
        "    IM_SIZE = (512, 512)\n",
        "    test_fn_2(ROOT_PATH + test_dataset, gdrive_saved_images_test = gdrive_saved_images_test, first = 'U-Net', second = 'Pix2Pix')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}